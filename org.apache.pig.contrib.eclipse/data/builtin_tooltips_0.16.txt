AVG=Computes the average of the numeric values in a single-column bag. 
BagToString=Concatenate the elements of a Bag into a chararray string, placing an optional delimiter between each value.
CONCAT=Concatenates two or more expressions of identical type.
COUNT=Computes the number of elements in a bag. 
COUNT_STAR=Computes the number of elements in a bag. 
DIFF=Compares two fields in a tuple.
IsEmpty=Checks if a bag or map is empty.
MAX=Computes the maximum of the numeric values or chararrays in a single-column bag. MAX requires a preceding GROUP ALL statement for global maximums and a GROUP BY statement for group maximums.
MIN=Use the MIN function to compute the minimum of a set of numeric values or chararrays in a single-column bag.
PluckTuple=Allows the user to specify a string prefix, and then filter for the columns in a relation that begin with that prefix.
SIZE=Computes the number of elements based on any Pig data type. 
SUBTRACT=Bags subtraction, SUBTRACT(bag1, bag2) equals to bags composed of bag1 elements not in bag2
SUM=Computes the sum of the numeric values in a single-column bag. SUM requires a preceding GROUP ALL statement for global sums and a GROUP BY statement for group sums.
TOKENIZE=Splits a string and outputs a bag of words. 
Handling Compression=Support for compression is determined by the load/store function. PigStorage and TextLoader support gzip and bzip compression for both read (load) and write (store). BinStorage does not support compression.
BinStorage=Loads and stores data in machine-readable format.
JsonLoader=Load or store JSON data.
JsonStorage=Load or store JSON data.
PigDump=Stores data in UTF-8 format.
PigStorage=Loads and stores data as structured text files.
TextLoader=Loads unstructured data in UTF-8 format.
HBaseStorage=Loads and stores data from an HBase table.
AvroStorage=Loads and stores data from Avro files.
TrevniStorage=Loads and stores data from Trevni files.
AccumuloStorage=Loads or stores data from an Accumulo table. The first element in a Tuple is equivalent to the "row" from the Accumulo Key, while the columns in that row are can be grouped in various static or wildcarded ways. Basic wildcarding functionality exists to group various columns families/qualifiers into a Map for LOADs, or serialize a Map into some group of column families or qualifiers on STOREs.
OrcStorage=Loads from or stores data to Orc file.
ABS=Returns the absolute value of an expression.
ACOS=Returns the arc cosine of an expression.
ASIN=Returns the arc sine of an expression.
ATAN=Returns the arc tangent of an expression.
CBRT=Returns the cube root of an expression.
CEIL=Returns the value of an expression rounded up to the nearest integer.
COS=Returns the trigonometric cosine of an expression.
COSH=Returns the hyperbolic cosine of an expression.
EXP=Returns Euler's number e raised to the power of x.
FLOOR=Returns the value of an expression rounded down to the nearest integer. 
LOG=Returns the natural logarithm (base e) of an expression.
LOG10=Returns the base 10 logarithm of an expression.
RANDOM=Returns a pseudo random number.
ROUND=Returns the value of an expression rounded to an integer.
ROUND_TO=Returns the value of an expression rounded to a fixed number of decimal digits.
SIN=Returns the sine of an expression.
SINH=Returns the hyperbolic sine of an expression.
SQRT=Returns the positive square root of an expression.
TAN=Returns the trignometric tangent of an angle.
TANH=Returns the hyperbolic tangent of an expression. 
ENDSWITH=Tests inputs to determine if the first argument ends with the string in the second. 
EqualsIgnoreCase=Compares two Strings ignoring case considerations. 
INDEXOF=Returns the index of the first occurrence of a character in a string, searching forward from a start index. 
LAST_INDEX_OF=Returns the index of the last occurrence of a character in a string, searching backward from the end of the string. 
LCFIRST=Converts the first character in a string to lower case. 
LOWER=Converts all characters in a string to lower case. 
LTRIM=Returns a copy of a string with only leading white space removed.
REGEX_EXTRACT =Performs regular expression matching and extracts the matched group defined by an index parameter. 
REGEX_EXTRACT_ALL =Performs regular expression matching and extracts all matched groups.
REPLACE=Replaces existing characters in a string with new characters.
RTRIM=Returns a copy of a string with only trailing white space removed.
SPRINTF=Use the SPRINTF function to format a string according to a template. For example, SPRINTF("part-%05d", 69) will return 'part-00069'.
STARTSWITH=Tests inputs to determine if the first argument starts with the string in the second. 
STRSPLIT=Splits a string around matches of a given regular expression. 
STRSPLITTOBAG=Splits a string around matches of a given regular expression and returns a databag
SUBSTRING=Returns a substring from a given string. 
TRIM=Returns a copy of a string with leading and trailing white space removed.
UCFIRST=Returns a string with the first character converted to upper case. 
UPPER=Returns a string converted to upper case. 
UniqueID=Returns a unique id string for each record in the alias. 
AddDuration=Use the AddDuration function to created a new datetime object by add some duration to a given datetime object.
CurrentTime=Returns the DateTime object of the current time.
DaysBetween=Returns the number of days between two DateTime objects.
GetDay=Returns the day of a month from a DateTime object.
GetHour=Returns the hour of a day from a DateTime object.
GetMilliSecond=Returns the millisecond of a second from a DateTime object.
GetMinute=Returns the minute of a hour from a DateTime object.
GetMonth=Returns the month of a year from a DateTime object.
GetSecond=Returns the second of a minute from a DateTime object.
GetWeek=Returns the week of a week year from a DateTime object.
GetWeekYear=Returns the week year from a DateTime object.
GetYear=Returns the year from a DateTime object.
HoursBetween=Returns the number of hours between two DateTime objects.
MilliSecondsBetween=Returns the number of milliseconds between two DateTime objects.
MinutesBetween=Returns the number of minutes between two DateTime objects.
MonthsBetween=Returns the number of months between two DateTime objects.
SecondsBetween=Returns the number of seconds between two DateTime objects.
SubtractDuration=Use the AddDuration function to created a new datetime object by add some duration to a given datetime object.
ToDate=Returns a DateTime object according to parameters.
ToMilliSeconds=Returns the number of milliseconds elapsed since January 1, 1970, 00:00:00.000 GMT for a DateTime object.
ToString=ToString converts the DateTime object to the ISO or the customized string.
ToUnixTime=Returns the Unix Time as long for a DateTime object. UnixTime is the number of seconds elapsed since January 1, 1970, 00:00:00.000 GMT.
WeeksBetween=Returns the number of weeks between two DateTime objects.
YearsBetween=Returns the number of years between two DateTime objects.
TOTUPLE=Converts one or more expressions to type tuple. 
TOBAG=Converts one or more expressions to type bag. 
TOMAP=Converts key/value expression pairs into a map. 
TOP=Returns the top-n tuples from a bag of tuples.
HiveUDAF=Pig invokes all types of Hive UDF, including UDF, GenericUDF, UDAF, GenericUDAF and GenericUDTF. Depending on the Hive UDF you want to use, you need to declare it in Pig with HiveUDF(handles UDF and GenericUDF), HiveUDAF(handles UDAF and GenericUDAF), HiveUDTF(handles GenericUDTF).
HiveUDF=Pig invokes all types of Hive UDF, including UDF, GenericUDF, UDAF, GenericUDAF and GenericUDTF. Depending on the Hive UDF you want to use, you need to declare it in Pig with HiveUDF(handles UDF and GenericUDF), HiveUDAF(handles UDAF and GenericUDAF), HiveUDTF(handles GenericUDTF).
HiveUDTF=Pig invokes all types of Hive UDF, including UDF, GenericUDF, UDAF, GenericUDAF and GenericUDTF. Depending on the Hive UDF you want to use, you need to declare it in Pig with HiveUDF(handles UDF and GenericUDF), HiveUDAF(handles UDAF and GenericUDAF), HiveUDTF(handles GenericUDTF).
Bloom=Bloom filters are a common way to select a limited set of records before moving data for a join or other heavy weight operation.